---
title: "Spike Analysis of Mice Visual Stimuli"
author: "Gianni Spiga"
date: "March 5, 2023"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    number_sections: yes
    theme: flatly
    code_folding: hide
editor_options: 
  markdown: 
    wrap: 72
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

```{r, messsage = F, warning = F, include = F}
library(ggplot2)
library(plotly) 
library(lme4)
library(knitr)
library(dplyr)
library(ggthemes) # theme_igray()
library(ggformula) #stat_spline
library(car)
```

# Abstract

[A (very) short summary of the report. As an example, you can simply
write one sentence for each section in the report.]{style="color:blue"}

# Introduction

The scale of complexity a a human brain has over that of a mouse is well
known among neuroscientists. The structure of a mouse brain is very
similar in structure and function to a human brain, thus making them an
interest of study in neurological testing. In this analysis, we look at
data from a recent experiment by Steinmetz et al. (2019) which observed
the activity of neurons in the visual cortex of mice when presented with
stimuli. Following this, the mice were required to make a decision,
using a wheel which was controlled by their forepaws. Their decision
foretold whether they received an award or a penalty.

# Background

The data set we will be using only looks at two of the ten mice in the
study, Cori and Forssmann from mid-December 2016 to early November 2017.
Both mice had varying number of trials, with 39 tracked times for each
trial. Since these five sessions have varying amounts of trials, we look
for a way to minimize the data from these five sessions into one long
data set. To do this, we first must create an accurate response variable
for our model, one that we can compare imbalanced trials between
sessions. We choose to compare the mean firing rates for each trial in
each session. This measure would be the average number of spikes per
seconds across all neurons measuring within a 0.4 second interval. The
choice of 0.4 seconds as our interval is by design of the experiment,
which focused on spike trains of neurons from the presenting of the
stimuli to 0.4 seconds after. The resulting data frame, from the five
merged sessions, is shown below. We have a data set of 1196 trials,
spanning across the 5 sessions, with the mean firing rate, the left and
right contrasts, feedback type, and an ID column for which session the
trial came from.

```{r, message = F, rows.print = 5}
ses1 <- readRDS("./Data/session1.rds")
ses2 <- readRDS("./Data/session2.rds")
ses3 <- readRDS("./Data/session3.rds")
ses4 <- readRDS("./Data/session4.rds")
ses5 <- readRDS("./Data/session5.rds")


session=list()
for(i in 1:5){
  session[[i]]=readRDS(paste('./Data/session',i,'.rds',sep=''))
  #print(session[[i]]$mouse_name)
  #print(session[[i]]$date_exp)
}


# id=11
# session[[5]]$feedback_type[id]
# session[[5]]$contrast_left[id]
# session[[5]]$contrast_right[id]
# length(session[[1]]$time[[id]])
# dim(session[[5]]$spks[[id]])

#ID=1
t=0.4 # from Background 

# Rows of contrast is quantity of trials (varying)
# rows of spikes/times is quantity of neurons (varying)
# columns of spikes/times is tracked sessions of brain (fixed)
# 
# n.trials=length(session[[ID]]$spks)
# # Number of nuerons is the rows of spikes. which varies 
# n.neurons=dim(session[[ID]]$spks[[1]])[1]
# 
# # Obtain the firing rate 
# firingrate=numeric(n.trials)
# for(i in 1:n.trials){
#   firingrate[i]=sum(session[[ID]]$spks[[i]])/n.neurons/t
# }
# firingrate

firingrate <- c()
total.fire.rate <- sapply(1:5, function(i)
  sapply(1:length(session[[i]]$spks), function(j)
    firingrate = c(firingrate, sum(session[[i]]$spks[[j]]) / dim(session[[i]]$spks[[j]])[1] / t)))

# Flatten the list of lists
rates.flat <- (unlist(total.fire.rate))
mice <- data.frame("Firing.Rate" = rates.flat)

# Now create dataframe to join with session as a variable as well
contrast.dat <- data.frame()
abc <- sapply(1:5, function(i)
  cbind(
    session[[i]]$contrast_left,
    session[[i]]$contrast_right,
    session[[i]]$feedback_type,
    rep(i, length(session[[i]]$contrast_left))
  ))
contrast.dat <- do.call(rbind, abc)

# Merge data.frames
mice <- cbind(mice, contrast.dat)
colnames(mice) <-
  c("Firing.Rate", "C.Left", "C.Right", "Feedback_Type", "Session")

# Convert mice Session into factor
cols2fac <- c("C.Left", "C.Right", "Feedback_Type", "Session")
mice[cols2fac] <- lapply(mice[cols2fac], factor)
mice
```

# Descriptive analysis

Before we begin our analysis, we will report some summary statistics of
the data as well as visualizations to understand our data with more
depth. Below we print a five number summary of our response, the mean
firing rate. We can see from the table that the lowest mean firing rate
is 0.404 and the largest is 7.219. However, with a median of 2.962, we
speculate the distribution of our mean firing rate will have some skew,
which we will plot below colored by Feedback Type and Session number.

```{r, warning = F, fig.size = "80%"}
kable(as.data.frame(c(summary(mice$Firing.Rate))))

# Create Histograms,
# separated by Session
ggplotly(
  ggplot(mice, aes(x = Firing.Rate, fill = Session)) + geom_histogram(
    bins = 22,
    alpha = 0.8,
    position = "identity"
  )  + theme_igray() + labs(
    x = "Firing Rate",
    y = "Count",
    fill = "Session",
    title = "Histograms of Firing Rates Colored by Session"
  )
)

# Separated by Feedback Type
ggplotly(
  ggplot(mice, aes(x = Firing.Rate, fill = Feedback_Type)) + geom_histogram(
    position = "dodge",
    bins = 15,
    alpha = 0.9
  ) + theme_igray() + scale_fill_manual(values = c("#56B4E9", "#E69F00", "#999999")) + labs(
    x = "Firing Rate",
    y = "Count",
    fill = "Feedback Type",
    title = "Histograms of Firing Rates Colored by Feedback Type"
  )
)

# Separated by Feedback Type, NOT OUTPUTTING
# ggplotly(ggplot(mice, aes(x = Firing.Rate, fill = Feedback_Type)) + geom_histogram(position = "identity", bins = 22, alpha = 0.5) + scale_fill_manual(values=c("#E69F00","#999999", "#56B4E9")))
```

The histograms of our firing rates show us with each session performed,
the distribution of firing rates shifts towards zero more and more. The
session with the highest firing rate is session 1 and the lowest firing
rate is session 5. Using the interactive plot to isolate each histogram
by session, we can also see that each session is heavily right skewed on
its own, except for Session 2. This session appears to have more
symmetry, with heavier tails. Separating by Feedback Type, we can see
more mice were given rewards than penalties for all firing rates except
those who had firing rates between 0.75 and 1, where there was just one
more ice penalized than rewarded.

Overall from these distributions, we can see our earlier suspicion was
correct regarding skew. The left portion of the histograms has more
observations than the right tails. While this skew is apparent, it is
not a strong departure, visually at least, from normality, which we will
assess *Sensitivity Analysis*.

```{r, warning = F, message = F}
# Box Plots 
# For Left Contrast
#ggplot(mice, aes(x = C.Left, y= Firing.Rate, fill = C.Left)) + geom_boxplot()
# Right Contrast
#ggplot(mice, aes(x = C.Right, y= Firing.Rate, fill = C.Right)) + geom_boxplot()
```

# Inferential analysis

Our goal is to build a mixed effects model for the firing rates of
neurons with fixed effects contrast left $\alpha_i$, contrast right
$\beta_j$, their interaction $\alpha\beta_{ij}$, and the random effect
$\gamma_k$ of each session. We have the following model:

$$ 
Y_{ijkl} = \mu + \alpha_i + \beta_j + \alpha\beta_{ij} + \gamma_k + \epsilon_{ijkl}
$$ where we have:$$
i = \{0, 0.25, 0.5, 1\} \\
j = \{0, 0.25, 0.5, 1\} \\
k = \{1 \ldots5\} \\
l = \{1 \ldots n_k\}
$$ With this model, we have a few constraints,$\sum_i n_i\alpha_i = 0$,
$\sum_j n_j\beta_j = 0$, $\sum_{ij} n_{ij}\alpha\beta_{ij} = 0$,
$\gamma \sim N(0, \sigma_{\gamma}^2)$, and
$\epsilon \sim N(0,\sigma^2)$. With fixed effects of both the left and
right contrasts, we aim to find how they affect the mean firing rate for
all neurons. However, because we are comparing these neuron mean firing
rates across multiple sessions, we must include a random effect measure
to account for all non-fixed effects that take place in switching
sessions. This random effect, as well as our error, are both normally
distributed.

We now build our model, using **lme4** library, which will allow us to
create the intercept $\gamma_k$ for each session in the model. Since we
will be performing model comparisons between fixed effects, we will
abstain from using the restricted maximum likelihood estimation (Oehlert
4). We first build a full model, as described above, where we receive
the following summary below.

```{r, fig.height = 6}
lm1 = lmer(Firing.Rate ~ C.Left * C.Right + (1 |Session),
           data = mice,
           REML = F)

# plot(lm1) save for sensitivity analysis
sum.lm1 <- summary(lm1)
kable(sum.lm1$coefficients)

#ranef(lm1)
#predict(lm1, )
```

From the summary, we can see from our fixed effects that as contrasts levels increase, the estimated mean firing rate increases. However, when we hold interactions between the left and right contrasts, the estimates lower, with all interactions being negative. We test for significance using the likelihood ratio test between our full interaction model and one with no interactions. 

```{r}
# Now build a model with no interactions 
lm2 = lmer(Firing.Rate ~ C.Left + C.Right + (1 |Session),
           data = mice,
           REML = F)
summary(lm2)

anova(lm1, lm2)

# From the likelihood ratio test we can see that with a p-value of 0.05, we reject the null, that there is no difference in the models, therefore we keep the full model 


### PERTAINING TO THE RANDOM EFFECT TESTING 
# The 3d Scatter will help visualize the difference between the random effects

# Set up data by predictions for 3d Scatter (not including heatmap, not as useful)
pred.mice <-
  expand.grid(x = unique(mice$C.Left), y = unique(mice$C.Right)) # All permutations of contrasts
pred.mice <- pred.mice %>% slice(rep(row_number(), 5))
pred.mice <-
  cbind(pred.mice, c(sapply(1:5, function (x)
    rep(x, 16)))) # combine with session numbers
colnames(pred.mice) <- c("C.Left", "C.Right", "Session")
pred.mice <-
  pred.mice %>% mutate_if(is.numeric, as.factor) # convert to factors
pred.mice <-
  cbind(pred.mice, predict(lm1, pred.mice)) # add predictions to df
colnames(pred.mice) <-
  c("C.Left", "C.Right", "Session", "Pred")  # add column names
# ggplotly(ggplot(pred.mice, aes(
#   x = C.Left, y = C.Right, fill = Pred
# )) + geom_tile() + facet_wrap( ~ Session, ncol = 2))

# Lets try with plotly scatter
l <- list(
  font = list(
    family = "sans-serif",
    size = 12,
    color = "#000"
  ),
  bgcolor = "#E2E2E2",
  bordercolor = "#FFFFFF",
  borderwidth = 2
)
plot_ly(
  x = pred.mice$C.Left,
  y = pred.mice$C.Right,
  z = pred.mice$Pred,
  type = "scatter3d",
  mode = "markers",
  color = pred.mice$Session
) %>% layout(
  scene = list(
    xaxis = list(title = "Left Contrast"),
    yaxis = list(title = "Right Contrast"),
    zaxis = list(title = "Mean Firing Rate")
  ),
  legend = list(title = list(text = '<b> Session </b>'))
) %>% layout(legend = l)


# Test for model with no random effect
lm.noRando <- lm(Firing.Rate ~ C.Left * C.Right,
           data = mice)
anova(lm.noRando)

# Likelihood ratio test 
anova(lm1, lm.noRando)

# Random effect has very small p-value, so we also reject and continue with our full mixed effect model
```

# Sensitivity Analysis

## Outliers

```{r}
# Before checking Normality and Homoscedasticity, lets check for outliers
cookD.lm1 <- cooks.distance(lm1)
row.ind <- seq(1, nrow(mice), 1)
ggplotly(
  ggplot() + aes(x = row.ind, y = cookD.lm1) + geom_point() + labs(y = "Cooks Distance", x = "Observation Number", title = "Plot of Cooks Distance vs Observation Number for Mixed Effect Model")
)

ggplotly(
  ggplot() + aes(x = row.ind, y = sum.lm1$residuals) + geom_point() + labs(y = "Cooks Distance", x = "Observation Number", title = "Plot of Residuals vs Observation Number for Mixed Effect Model")
)

# From the plot we can see that observations 10, 806, and 3 have the highest Cook's distance, lets see what stands out
# 44 has the largest residual value 
mice[c(3,10,44, 806),]

# Observation 10 has a firing rate of 6.66, when the average firing rate is around 4, Observation 3 is also a session 1 with a high firing rate, however it is not as drastic as observation 10
# 44 is an extremely high mean firing rate for session 1
# Observation 806 has an abnormally high firing rate for the left and right contrast it has in its session, the predicted value for a cleft 0.25 and cright 0 is 2.183 where observation 806 is 3.85 firing rate

# We will remove these rows, rebuild the model, and check assumptions
mice.red <-  mice%>%
  filter(!row_number() %in% c(3, 10,44, 806))

# build model with outliers removed
lm1.red = lmer(Firing.Rate ~ C.Left * C.Right + (1 |Session),
           data = mice.red,
           REML = F)
sum.lm1.red <- summary(lm1.red)
# Continue with sensitivity analysis
```

## Normality

```{r}
# Plot a histogram of the residuals 
ggplotly(
  ggplot() + aes(x = sum.lm1.red$residuals) + geom_histogram(bins = 20, fill = "#E69F00") + theme_igray() + labs(x = "Pearson Residuals", y = "Count", title = "Distribution of Residuals for Full Mixed Effect Model")
)

# Shapiro.test
shapiro.test(sum.lm1.red$residuals)
```

## Homoscedasticity

```{r}
ggplotly(
  ggplot() + aes(x = predict(lm1.red), y = sum.lm1.red$residuals) + geom_point() + theme_igray() + stat_spline(
    spar = 0.95,
    size = 0.8,
    color = "#cd534cff"
  ) + labs(x = "Fitted Values", y = "Pearson Residuals", title = "Pearson Residuals vs. Fitted Values for the Final Model")
)

head(mice.red)
leveneTest(Firing.Rate ~ C.Left*C.Right, data = mice.red)

# We can see from our Levene Test and the fitted values vs residuals the assumption is met
```

# Predictive Modeling

```{r}
# We now focus on building a logistic regression model for Feedback type, we will start with full data 
mice.logit <-
  glm(Feedback_Type ~ C.Left + C.Right + Session,
      data = mice,
      family = "binomial")
summary(mice.logit)


```

# Discussion

[Conclude your analysis in this section. You can touch on the following
topics.]{style="color:blue"}

-   A brief recap of this project.
-   Suggestions for future research and/or policy making given your
    findings.
-   Caveats of the current analysis.

# Predictive Modeling

# Acknowledgement {.unnumbered}

[By default, it is assumed that you have discussed this project with
your teammates and instructors. List any other people that you have
discussed this project with.]{style="color:blue"}

# Reference {.unnumbered}

[List any references you cited in the report. See
[here](https://owl.purdue.edu/owl/research_and_citation/apa_style/apa_formatting_and_style_guide/in_text_citations_the_basics.html)
for the APA format, as an example:]{style="color:blue"}

Imbens, G., & Rubin, D. (2015). Stratified Randomized Experiments. In
Causal Inference for Statistics, Social, and Biomedical Sciences: An
Introduction (pp. 187-218). Cambridge: Cambridge University Press.
<doi:10.1017/CBO9781139025751.010>

# Session info {.unnumbered}

[Report information of your `R` session for
reproducibility.]{style="color:blue"}

```{r}
sessionInfo()
```
