---
title: "Homework 7"
author: "Gianni Spiga"
date: '2022-11-17'
output: 
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    df_print: paged
---
## Question 1
### a.) 
False, we would not need 3 dummy variables, only 2. If we did 3, we would be creating a linear combination of the first two columns. This is known as the dummy variable trap. 

### b.) 
False, we do not prefer such high order models because they have high sampling variability and risk overfitting the data. 

### c.) 
True, since the terms are multiplied, the scale of the interaction depends on the value of the terms multipled. 

### d.) 
False, separate models, while more flexible, are not as efficient and have more sampling variablity than a joint model. 


## Question 2
```{r, message = F}
# Libraries
library(MASS)
library(ggplot2)
library(plotly)
library(GGally)
```
### a.) 
```{r}
cars <- read.csv("Cars.csv")
cars
```
### b.)
```{r}
# Fixing horsepower column
cars$horsepower <- as.numeric(cars$horsepower)

# Find index of NA rows
NArows <- which(is.na(cars))

# View rows
cars[NArows,]
```

### c.) 
```{r}
str(cars)
```

We should treat the variables "cylinders" and "country.code" as categorical variables. The rest of the variables should be treated as quantitative variables. 

```{r}
cars$cylinders <- as.factor(cars$cylinders)
cars$country.code <- as.factor(cars$country.code)
```

### d.) 
```{r}
# MPG
ggplot(data = cars, aes(x = mpg)) + geom_histogram(bins = 15)

#Displacement
ggplot(data = cars, aes(x = displacement)) + geom_histogram(bins = 10)

# Horsepower
ggplot(data = cars, aes(x = horsepower))+ geom_histogram(bins = 20)

# Weight
ggplot(data = cars, aes(x = weight)) + geom_histogram()

# Acceleration
ggplot(data = cars, aes(x = acceleration)) + geom_histogram()
```

From the histograms we notice the following: is slightly right skewed, displacement is heavily right skewed, horsepower and weight is moderately right skewed, and acceleration is approximately normal. 

### e.) 
```{r, message = F, warning = F}
ggpairs(cars[,c(1, 3, 4, 5, 6)], title = "Correlogram of Cars Data")
```

We can see from the scatter plot matrix that we face multicollinearity with predictors in this data. The variable mpg, displacement, horsepower, and weight are all highly correlated with each other, with the highest being the correlation between displacement and weight with a correlation of $0.933$. 

### f.)
```{r}
library(plotly)
fig1 <-
  plot_ly(
    data = data.frame(table(cars$cylinders)),
    labels = ~ Var1,
    values = ~ Freq,
    type = 'pie'
  )
fig1 <-
  fig1 %>% layout(
    title = "Number of Cylinders",
    xaxis = list(
      showgrid = FALSE,
      zeroline = FALSE,
      showticklabels = FALSE
    ),
    yaxis = list(
      showgrid = FALSE,
      zeroline = FALSE,
      showticklabels = FALSE
    )
  )
fig1

fig2 <-
  plot_ly(
    data = data.frame(table(cars$country.code)),
    labels = ~ Var1,
    values = ~ Freq,
    type = 'pie'
  )
fig2 <-
  fig2 %>% layout(
    title = "Pie Chart of Country Code",
    xaxis = list(
      showgrid = FALSE,
      zeroline = FALSE,
      showticklabels = FALSE
    ),
    yaxis = list(
      showgrid = FALSE,
      zeroline = FALSE,
      showticklabels = FALSE
    )
  )
fig2
```

### g.) 
```{r}
ggplot(data = cars, aes(x = cylinders, y = mpg, fill = country.code)) + geom_boxplot()
```

We can seet that only cylinder options that were available in all three countries were 4 cylinders and 6 cylinders. Some countries only had one cylinder option. Four cylinder cars performed, on average, the best in miles per gallon, regardless of the country. Eight cylinder cars had the worse mpg performance. Four cylinder cars in country code 3 performed the best of all other categories on average. 

## Question 3

### a.) 
```{r}
ggplot(data = cars, aes(x = mpg)) + geom_histogram(bins = 20)

cars.Model <- lm(mpg ~ cylinders + horsepower + weight, data = cars)

bc <- boxcox(cars.Model)

``` 
From the histogram and the boxcox plot, we can see that mpg is moderately right skewed, because of this, we will perform a transformation chosen by the boxcox transformaton. 

```{r}
# Find best lambda
lambda <- bc$x[which.max(bc$y)]

# make new data column
cars$mpgTrans <- (cars$mpg^lambda - 1)/ lambda
```

### b.) 

```{r}
cars.Model.T <- lm(mpgTrans ~ cylinders + horsepower + weight, data = cars)
summary(cars.Model.T)
```
From the F test, we can see a very small p-value, indicating that at least one of the coefficients is significant. Both weight and horsepower are highly significant. Cars with 4 and 5 cylinders are significant however those with 6 and 8 cylinder engines are not. The $R^2$ and $R^2_{adj}$ are 0.8223 and 0.8196 respectively, indicating a strong fit. The model is adequite. 

### c.) 

$$
\hat{Y_i} = (2.018 + 0.07577) - 0.00085X_{hp} - 0.000061X_{wt}
$$
### d.)
```{r}
cars.Model.Int <- lm(mpgTrans ~ cylinders + horsepower + weight + cylinders*horsepower + cylinders*weight, data = cars)
summary(cars.Model.Int)
``` 

$$
\hat{Y_i} = (-3.418 + 5.521) + (0.263 - 0.265)X_{hp} + (-0.009 + 0.009)0.000061X_{wt}
$$

### e.) 
```{r}
anova(cars.Model.Int, cars.Model.T)
```
We can see from the anova function, we would reject $H_0$ concluding that the full model with interactions is a better fit. 

### f.) 
```{r}
# For non interaction model
predict(cars.Model.T, data.frame(cylinders = "4", horsepower = 100, weight = 3000), interval = "prediction", level = 0.95)

# With interaction model
predict(cars.Model.Int, data.frame(cylinders = "4", horsepower = 100, weight = 3000), interval = "prediction", level = 0.95)
```

We find that both predictions are almost identical to each other for both models. 